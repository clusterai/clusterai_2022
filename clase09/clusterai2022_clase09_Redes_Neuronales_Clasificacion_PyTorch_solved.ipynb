{"cells":[{"cell_type":"markdown","metadata":{"id":"BHzfMHbFqIKi"},"source":["# ClusterAI 2022\n","\n","# Ciencia de Datos - Ingeniería Industrial - UTN BA\n","\n","# clase_09: Practica Redes Neuronales\n","\n","### Elaborado por: Aguirre Nicolas"]},{"cell_type":"markdown","metadata":{"id":"S8Y4IUeNrTH4"},"source":["# IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zj2VEgN6qjDJ"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","pd.set_option('display.float_format', lambda x: '%.1d' % x) # Para acotar los decimales en pandas"]},{"cell_type":"markdown","metadata":{"id":"shZj2OGKs0Hl"},"source":["# IRIS DATASET\n","\n","En esta primera ejercitacion vamos a retomar el dataset Iris (visto en la Clase 4)\n","\n","\n","El conjunto de datos contiene 50 muestras de cada una de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midió cuatro rasgos de cada muestra: lo largo y lo ancho del sépalos y pétalos, en centímetros. Basado en la combinación de estos cuatro rasgos.\n","\n","https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos\n","\n","Los datos son:\n","\n","| Columna | Descripcion |\n","| --- | --- |\n","| ID | Unique ID |\n","| SepalLengthCm | Length of the sepal (cm) |\n","| SepalWidthCm | Width of the sepal (cm) |\n","| PetalLengthCm | Length of the petal (cm) |\n","| PetalWidthCm | Width of the petal (cm) |\n","| Species | name |\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxUnNijVsmww"},"outputs":[],"source":["# Primero cargamos los datos que ya vienen incluidos en la libreria sk-learn.\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","\n","X = iris.data\n","Y = iris.target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fIbBihoeU95Q"},"outputs":[],"source":["n_features = np.shape(X)[0]\n","n_samples = np.shape(X)[1]\n","n_classes = np.unique(Y)\n","print(f'Features: ',n_samples)\n","print(f'Samples: ',n_features)\n","print(f'Classes: ',n_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hhudQ_Xs1yL"},"outputs":[],"source":["# Veamos la primera sample\n","print(f'X: {X[0]}')\n","print(f'Y: {Y[0]}')"]},{"cell_type":"markdown","metadata":{"id":"--P7frv5qX51"},"source":["## SPLIT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSP2IQWiq91_"},"outputs":[],"source":["# Separamos train y test set\n","x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n","# Separamos train y validation set\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"SFMeW20jqa52"},"source":["## SCALING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UReu6invWAGg"},"outputs":[],"source":["# Noralizamos\n","scaler = preprocessing.StandardScaler()\n","scaler.fit(x_train)\n","x_train_norm = scaler.transform(x_train)\n","x_val_norm = scaler.transform(x_val)\n","x_test_norm = scaler.transform(x_test)"]},{"cell_type":"markdown","metadata":{"id":"oPCxvrLMtFYc"},"source":["# Modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dv5XmStNuTP0"},"outputs":[],"source":["# Pytorch\n","import torch\n","print('Version de Pytorch: ',torch.__version__)\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset,Dataset, DataLoader\n","import torch.nn.functional as F\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"x4wHwXuVtPg9"},"source":["## Modelo"]},{"cell_type":"markdown","metadata":{"id":"LJfJ3XbtEfoa"},"source":["Al igual que TensorFlow-Keras, PyTorch es una libreria para codear modelos de NN y tambien tiene su modelo Sequential.\n","\n","Pero a diferencia de TF-Keras, PyTorch no tiene nativamente las funciones **fit**, **evaluate** y **predict**.\n","\n","Por lo cual somos nosotros quienes vamos a tener codear el entrenamiento ..."]},{"cell_type":"markdown","metadata":{"id":"3_L2XfJcEF9Q"},"source":["El tipo de arquitectura que vamos a utilizar es una NN *fully-conected*. Para esto utilizaremos la funcion *nn.Sequential()* a la cual le pasaremos los distintos componentes de la red.\n","\n","Cuando usamos una arquitectura secuencial, vamos construyendo el **foreward pass** de la red de manera tal que la red utiliza la salida de la capa inmediatamente anterior $z^{l-1}$ en la capa siguiente z^{l}, y asi sucecivamente hasta llegar al ultimo elemento de la red con el cual generamos el output.\n","\n","$$\n","z^{l} = \\sigma(Wz^{l-1}+b)\n","$$\n","\n","Puntualmente, en este ejemplo utilizaremos unicamente dos componentes.\n","\n","* [**nn.Linear**](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) : Para aplicar la transformacion lineal $\\delta = Wx + b$, \n","donde $\\delta$ es llamada tambien *pre-activacion* de la neurona.\n","\n","* [**nn.Sigmoid**](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) : Para aplicar la funcion $\\sigma(\\delta) = \\frac{1}{1 + \\exp(-\\delta)}$\n","\n","En donde $W$ son los pesos (weights) que aprendera la red.\n","\n","\n","Cada una de las capas de nn.Linear se construye dandole la informacion de la cantidad de features de entrada y de salida. Como en nuestro ejemplo tenemos 3 clases de flores, la ultima capa debera tener una salida de dimension = 3."]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://drive.google.com/uc?id=1SVhzQcrNGipyVGf7tDpfMkjICQEPwNgg\" width=\"1200\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdKlnGr2tFEd"},"outputs":[],"source":["input_features = 4\n","layers_features = 4\n","output_dim = 3\n","model = nn.Sequential(\n","          nn.Linear(input_features,layers_features),\n","          nn.Sigmoid(),\n","          nn.Linear(layers_features,layers_features),\n","          nn.Sigmoid(),\n","          nn.Linear(layers_features,layers_features),\n","          nn.Sigmoid(),\n","          nn.Linear(layers_features,output_dim)\n","          )         "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90n1GM9SvPUK"},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://drive.google.com/uc?id=1TlsED3rhvczkLSSrN7XPb3fSCO4rH32S\" width=\"1200\">"]},{"cell_type":"markdown","metadata":{"id":"RlSnpr9ICHOA"},"source":["## Optimizador y Loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QH9v8SqvYuX"},"outputs":[],"source":["# Learning rate y Optimizador\n","lr = 0.05\n","optimizador = torch.optim.SGD(model.parameters(),lr=lr)\n","# Funcion de penalizacion\n","loss_func = nn.CrossEntropyLoss() "]},{"cell_type":"markdown","metadata":{"id":"GQqCg3n74MbG"},"source":["# Tensor Dataset"]},{"cell_type":"markdown","metadata":{"id":"xse7iLVFH84T"},"source":["En general, cuando entrenamos NN los datos deben estar contenidos en tensores. \n","Un tensor es una generalización de los vectores y las matrices y se entiende fácilmente como una matriz multidimensional. \n","\n","Ejemplo:\n","\n","1. Un vector es un 1D tensor de dimension $1x5$\n","\n","2. Una matriz es un 2D tensor de dimension $2x5$\n","\n","En particular, dentro del área de Deep Learning se llama Tensor a aquellas estructura de datos que son capaces de realizar operaciones en **paralelo** dentro de las GPUs, aumentando significativamente nuestra capacidad de computo."]},{"cell_type":"markdown","metadata":{"id":"4YcFpOAjK99Y"},"source":["Por ultimo deberemos definir el generador de datos para entrenar en batches. Estos objetos se llaman Dataloaders. \n","\n","Para crearlo le pasamos nuestro dataset y el tamaño de nuestro batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrWi1A7GwDFB"},"outputs":[],"source":["# Batch size\n","bs = 8\n","\n","# Pasamos nuestro numpy arrays, a Tensor\n","x_train_norm_t, y_train_t = torch.Tensor(x_train_norm), torch.LongTensor(y_train) # Los Long Tensor son int\n","x_val_norm_t, y_val_t = torch.Tensor(x_val_norm), torch.LongTensor(y_val)\n","x_test_norm_t, y_test_t = torch.Tensor(x_test_norm), torch.LongTensor(y_test)\n","\n","# Creamos los Dataset\n","train_ds = TensorDataset(x_train_norm_t,y_train_t)\n","val_ds = TensorDataset(x_val_norm_t,y_val_t) \n","test_ds = TensorDataset(x_test_norm_t,y_test_t) \n","\n","# Creamos los Dataloader\n","train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n","val_dl = DataLoader(val_ds, batch_size=bs, shuffle=True) \n","test_dl = DataLoader(test_ds, batch_size=bs, shuffle=True) "]},{"cell_type":"markdown","metadata":{"id":"eCFJnJ86Bpmt"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"v65OgSpRGM0q"},"source":["En este punto ya contamos con todas las \"herramientras\" que necesitamos para entrenar un modelo de NN.\n","\n","* Datos\n","* Optimizador\n","* Funcion de Penalizacion\n","* Red Neuronal\n","\n","Vamos a ver como es que esto elementos \"interactuan\" para actualizar los parametros/*weights* de la Red Neuronal ... es decir ... para entrenarla."]},{"cell_type":"markdown","metadata":{"id":"pAwFcarcVMsH"},"source":["<img src=\"https://drive.google.com/uc?id=1d7Rq5FsmQNWcP8T1KGi_TodL49jCEKnY\" width=\"1200\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NapSjQIkwDD4"},"outputs":[],"source":["# Cantidad de Epochs\n","n_epochs = 900\n","\n","# Generamos un diccionario donde vamos a guaradr historial de entrenamiento\n","training = {'train':{'loss':[],                             \n","                    'acc':[]},\n","           'val':{'loss':[],\n","                  'acc':[]}\n","           }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJx1qPJ3wDAu"},"outputs":[],"source":["################################################\n","# TRAINING: Epochs\n","################################################\n","for epoch in range(n_epochs):\n","  # Ahora nosotros vamos a tener que definir estas variables ...\n","  train_loss = 0\n","  cls_correctas_train = 0\n","  train_acc = 0\n","  ################################################\n","  # TRAINING: Batch\n","  ################################################\n","  for i , (x_batch, y_batch) in enumerate(train_dl):\n","    # Limpiamos todos los gradientes cargados en el optimizador\n","    optimizador.zero_grad()\n","    # Con un x_batch generamos una prediccion\n","    y_pred = model(x_batch)\n","    # La clase predicha será el índice de máximo valor luego del \n","    # softmax (integrado en CrossEntropyLoss)\n","    _, predicted = torch.max(y_pred.data, 1)\n","    # Calculamos la loss\n","    batch_loss = loss_func(y_pred,y_batch)\n","    # Calculamos el gradiente de la loss\n","    # Aca es donde sucede el back-propagation a.k.a. \"la magia\"\n","    batch_loss.backward() \n","    # Ajustamos los parametros del modelo con el optimizador\n","    optimizador.step()\n","    # Acumulamos la loss\n","    train_loss += batch_loss.item()\n","    # Sumamos la cantidad de clases correctas en el batch\n","    correct_i = (predicted == y_batch).sum().item()\n","    # Acumulamos corrects\n","    cls_correctas_train += correct_i\n","\n","  ################################################\n","  # TRAINING: Validation\n","  ################################################\n","  val_loss = 0\n","  cls_correctas_val = 0\n","  val_acc = 0\n","  with torch.torch.inference_mode():\n","    for x_batch, y_batch in val_dl:\n","      y_pred = model(x_batch)\n","      _, predicted = torch.max(y_pred.data, 1)\n","      batch_loss = loss_func(y_pred,y_batch)\n","      val_loss += batch_loss.item()\n","      correct_i = (predicted == y_batch).sum().item()\n","      cls_correctas_val += correct_i\n","\n","  # Calculamos el accuracy\n","  train_acc = (cls_correctas_train / len(train_ds))*100    # Calculamos el accuracy\n","  val_acc = (cls_correctas_val / len(val_ds))*100  \n","\n","  # Imprimimos en pantalla\n","  print('Epoch: {} T_Loss: {:.4f} T_Acc: {:.2f}%  V_Loss: {:.4f} V_Acc: {:.2f} %'.format(\n","      epoch,train_loss,train_acc,val_loss,val_acc))\n","\n","  # Guardamos en el historial de entrenamiento ...\n","  training['train']['loss'].append(train_loss)\n","  training['train']['acc'].append(train_acc)\n","  training['val']['loss'].append(val_loss)\n","  training['val']['acc'].append(val_acc)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jcTsBG9UAYJe"},"outputs":[],"source":["#Loss\n","train_loss_h = training['train']['loss']\n","val_loss_h = training['val']['loss']\n","# Acc\n","train_acc_h = training['train']['acc']\n","val_acc_h = training['val']['acc']\n","lepochs = range(1, len(train_loss_h) + 1)\n","\n","fig,(axs1,axs2) = plt.subplots(1,2,figsize=(16,6))\n","axs1.plot(lepochs, train_loss_h, 'b', label='Train loss')\n","axs1.plot(lepochs, val_loss_h, 'r', label='Val loss')\n","axs1.set_title('Training and validation Loss',fontsize=20)\n","axs1.set_ylabel('Loss',fontsize=16)\n","axs1.legend(fontsize=16)\n","axs2.plot(lepochs, train_acc_h, 'b', label='Train Accuracy')\n","axs2.plot(lepochs, val_acc_h, 'r', label='Validation Accuracy')\n","axs2.set_title('Training and validation Accuracy',fontsize=20)\n","axs2.set_ylabel('Accuracy [%]',fontsize=16)\n","axs2.legend(fontsize=16)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"79yqU-Uunu5O"},"source":["# **PREGUNTAS**:\n","```\n","1) Es correcto que nuestra loss de train y de validacion disminuyan, y sin embargo el accuracy se mantenga igual/baje ? Por que ? \n","\n","2) Que cambios podriamos hacer para intentar solucionar el entrenamiento con la sigmoid function?\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"US03jef4exdE"},"source":["# DESAFIO"]},{"cell_type":"markdown","metadata":{"id":"PvxWJk1ve6VV"},"source":["Generar una funcion llamada **func_train()** y **func_val()** para reemplezar en el for loop de entrenamiento.\n","\n","Luego, generar una fucion llamada **func_fit()** la cual reemplace todo el entrenamiento y validacion (debe contener dentro de si **func_train()** y **func_val()** ).\n","\n","Ayuda: \n","\n","func_fit() debe recibir como argumentos (inputs)\n","\n","* el modelo, optimizador, loss function, dataloaders, datasets y la cantidad de epochs a entrenar\n","\n","* el return de la funcion es el modelo ya entrenado y el historial de entrenamiento.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oF8i9BUie-kD"},"outputs":[],"source":["def func_train(model, train_dl,train_ds, loss_func, optimizador):\n","  train_loss = 0\n","  cls_correctas_train = 0\n","  train_acc = 0\n","  ################################################\n","  # TRAINING\n","  ################################################\n","  for i , (x_batch, y_batch) in enumerate(train_dl):\n","    # Limpiamos todos los gradientes cargados en el optimizador\n","    optimizador.zero_grad()\n","    # Con un x_batch generamos una prediccion\n","    y_pred = model(x_batch)\n","    # La clase predicha será el índice de máximo valor luego del \n","    # softmax (integrado en CrossEntropyLoss)\n","    _, predicted = torch.max(y_pred.data, 1)\n","    # Calculamos la loss\n","    batch_loss = loss_func(y_pred,y_batch)\n","    # Calculamos el gradiente de la loss\n","    # Aca es donde sucede el back-propagation a.k.a. \"la magia\"\n","    batch_loss.backward() \n","    # Ajustamos los parametros del modelo con el optimizador\n","    optimizador.step()\n","    # Acumulamos la loss\n","    train_loss += batch_loss.item()\n","    # Sumamos la cantidad de clases correctas en el batch\n","    correct_i = (predicted == y_batch).sum().item()\n","    # Acumulamos corrects\n","    cls_correctas_train += correct_i\n","  # Calculamos el accuracy\n","  train_acc = (cls_correctas_train / len(train_ds))*100    # Calculamos el accuracy\n","\n","  return (model, train_loss,  cls_correctas_train,  train_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNJUtIYyhkzO"},"outputs":[],"source":["def func_validation(model, val_dl,val_ds,loss_func):\n","  val_loss = 0\n","  cls_correctas_val = 0\n","  val_acc = 0\n","  ################################################\n","  # VALIDATION\n","  ################################################\n","  with torch.torch.inference_mode():\n","    for x_batch, y_batch in val_dl:\n","      y_pred = model(x_batch)\n","      _, predicted = torch.max(y_pred.data, 1)\n","      batch_loss = loss_func(y_pred,y_batch)\n","      val_loss += batch_loss.item()\n","      correct_i = (predicted == y_batch).sum().item()\n","      cls_correctas_val += correct_i\n","  # Calculamos el accuracy\n","  val_acc = (cls_correctas_val / len(val_ds))*100\n","  return (val_loss, cls_correctas_val, val_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ufv-Lyqhms-"},"outputs":[],"source":["def func_fit(n_epochs,model,\n","             train_dl,train_ds,val_dl,val_ds,\n","             loss_func,optimizador):\n","  \n","  # Generamos un diccionario donde vamos a guaradr historial de entrenamiento\n","  training = {'train':{'loss':[],                             \n","                      'acc':[]},\n","            'val':{'loss':[],\n","                    'acc':[]}\n","            }\n","  # For loop epochs\n","  for epoch in range(n_epochs):\n","    model,train_loss, cls_correctas_train, train_acc =  func_train(model, train_dl,train_ds, loss_func, optimizador)\n","    val_loss, cls_correctas_val, val_acc = func_validation(model, val_dl,val_ds,loss_func)\n","    # Imprimimos en pantalla\n","    print('Epoch: {} T_Loss: {:.4f} T_Acc: {:.2f}%  V_Loss: {:.4f} V_Acc: {:.2f} %'.format(\n","        epoch,train_loss,train_acc,val_loss,val_acc))\n","\n","    # Guardamos en el historial de entrenamiento ...\n","    training['train']['loss'].append(train_loss)\n","    training['train']['acc'].append(train_acc)\n","    training['val']['loss'].append(val_loss)\n","    training['val']['acc'].append(val_acc)\n","  return (model,training)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77gbWM6CiwAs"},"outputs":[],"source":["# Cantidad de Epochs a entrenar\n","n_epochs = 800 \n","\n","model,training = func_fit( n_epochs,model,train_dl,train_ds,val_dl,val_ds,loss_func,optimizador)"]},{"cell_type":"markdown","metadata":{"id":"ULAa37jNjq8q"},"source":["# Creacion del Modelos \n","## (Avanzado y No Obligatorio para el curso !)\n","En la siguiente seccion veremos como se definen los modelos de NN de un modo mas avanzado.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0D7l-1x0lLw6"},"outputs":[],"source":["class Red_Neuronal_Personalizada(nn.Module):\n","    def __init__(self,input_features,layers_features,output_dim):\n","        super(Red_Neuronal_Personalizada, self).__init__()\n","        # Primera Capa\n","        self.layer_1 = nn.Linear(input_features,layers_features)\n","        self.act_1 = nn.Sigmoid()\n","        # Segunda Capa\n","        self.layer_2 = nn.Linear(layers_features,layers_features)\n","        self.act_2 = nn.Sigmoid()\n","        # Tercera Capa\n","        self.layer_3 = nn.Linear(layers_features,layers_features)\n","        self.act_3 = nn.Sigmoid()\n","        # Ultima Capa\n","        self.layer_4 = nn.Linear(layers_features,output_dim)    \n","\n","    def forward(self, x):\n","        z = self.act_1(self.layer_1(x))\n","        z = self.act_2(self.layer_2(z))\n","        z = self.act_3(self.layer_3(z))\n","        output = self.layer_4(z)\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"4L3Sk8EtlQ94"},"source":["Esto a primera vista puede ser un poco abrumador, pero vayamos viendo de a poco que es lo que hace cada parte del codigo.\n","\n","\n","\n","```python\n","class Red_Neuronal_Personalizada(nn.Module):\n","```\n","En esta parte definimos una clase que llamamos **Red_Neuronal_Personalizada**, que nos permitira luego crear unaobjetos/instancias de dicha clase. \n","\n","```python\n","modelo = Red_Neuronal_Personalizada()\n","```\n","Es similiar a cuando definimos una funcion, solo que es con objetos de PyThon.\n","\n","```python\n","def func_propia(): \n","\n","class clase_propia(): \n","```\n","\n","Con respecto a la siguiente parte del código\n","```python\n","    def __init__(self):\n","        super(Red_Neuronal_Personalizada, self).__init__()\n","```        \n","\n","Simplemente le estamos diciendo de que partes/atributos estará compuesta nuestra clase/modelo cuando lo creemos. \n","\n","La funcion __init__ es una funcion interna de Python que se ejecuta cuando creamos una instancia de nuestro objeto. Es decir, cuando ejecutemos \n","\n","```python\n","modelo = Red_Neuronal_Personalizada()\n","```\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"C4kuQ6KulfZI"},"source":["\n","Hasta este punto las partes de nuestro modelo aun no las hemos definido, asi que es lo que haremos en la siguientes lineas:\n","\n","```python\n","class Red_Neuronal_Personalizada(nn.Module):\n","    def __init__(self):\n","        super(Red_Neuronal_Personalizada, self).__init__()\n","        # Primera Capa\n","        self.layer_1 = nn.Linear(input_features,layers_features)\n","        self.act_1 = nn.Sigmoid()\n","        # Segunda Capa\n","        self.layer_2 = nn.Linear(layers_features,layers_features)\n","        self.act_2 = nn.Sigmoid()\n","        # Tercera Capa\n","        self.layer_3 = nn.Linear(layers_features,layers_features)\n","        self.act_3 = nn.Sigmoid()\n","        # Ultima Capa\n","        self.layer_4 = n.Linear(layers_features,output_dim)\n","```\n","\n","* self. : Siempre que definimos las partes (formalmente llamado **atributo**) de las que estara compuesta un objeto en python, se **TIENE** que anteponer la palabra **self.nombre_parte**. De esta manera, cuando querramos que nuestro objeto utilice de alguna manera dicha parte, lo indicaremos de la misma manera: **self.nombre_parte()**\n","\n","En este ejemplo concreto, nuestro modelo / objeto tendra 7 componentes:\n","\n","* Layers 1, 2, 3 y 4\n","* Activaciones 1, 2 y 3\n","\n","Todas estas componentes son objetos que importamos de la libreria PyTorch.\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"QcYWFrw0uGUF"},"source":["Veamos la siguiente linea de nuestro codigo:\n","```python\n","def forward(self, x):\n","```  \n","\n","En esta seccion definiremos el forward pass nosotros mismo. Como vemos, forward es una funcion que pertenecera a nuestro objeto y se utilizara cuando durante el entrenamiento pasemos la siguiente info.\n","\n","```python \n","prediccion = model (x_batch)\n","```\n"," **x_batch** en esta parte hace referencia a la **x** en:\n","\n","```python \n","def forward(self, x):\n","```\n","\n","y *self*, se lo pasaremos SIEMPRE que definimos una funcion en nuestro objeto que necesitemos que tenga acceso a los atributos que definimos en la seccion ____ init ____\n","\n","Bueno, sabiendo ya lo que haremos en esta funcion, pasemos a la siguiente parte del codigo ...\n"]},{"cell_type":"markdown","metadata":{"id":"bBk0ZHR0wKN9"},"source":["```python \n","    def forward(self, x):\n","        z = self.act_1(self.layer_1(x))\n","        z = self.act_2(self.layer_2(z))\n","        z = self.act_3(self.layer_3(z))\n","        output = self.layer_4(z)\n","        return output\n","```        "]},{"cell_type":"markdown","metadata":{"id":"EcuY6FB0wMM0"},"source":["En cada una de las lineas, obtenemos la salida de cada una de las capas. Fijense en particular que llamamos a las partes/atributos con **self.**.\n","\n","En particular : \n","```python \n","z = self.act_1(self.layer_1(x))\n","```\n","\n","Equivale a:       $z = \\sigma ( W \\cdot X + b)$\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"WMGV76WawvA-"},"source":["Con esto ultimo quedo definida la arquitectura de nuestra red!\n","\n","Vamos ahora a crear una instancia!"]},{"cell_type":"markdown","metadata":{"id":"z2DobiImxwav"},"source":["Si observamos nuevamente en la funcion que genera los objetos\n","\n","\n","\n","\n","```python \n","def __init__(self,input_features,layers_features,output_dim):\n","```\n","\n","Le pasamos los argumentos:\n","\n","* input_features\n","* layers_features\n","* output_dim\n","\n","Asi que vamos a definirlos para crear una intancia llamada **modelo_personalizado** de nuestra clase **Red_Neuronal_Personalizada**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vbs50MUflbnA"},"outputs":[],"source":["input_features = 4\n","layers_features = 4\n","output_dim = 3\n","modelo_personalizado = Red_Neuronal_Personalizada(input_features,layers_features,output_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zfimw41hyUkG"},"outputs":[],"source":["type(modelo_personalizado)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNE7Ox0I09z7"},"outputs":[],"source":["modelo_personalizado"]},{"cell_type":"markdown","metadata":{"id":"WRSXzvUNy1Sr"},"source":["Listo! \n","\n","Ahora entendemos que es lo que suecede internamente en **nn.Sequential()**, con la gran diferencia que ahora tenemos plena capacidad para definir las partes de nuestra red, su interaccion y el **forward pass**.\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"clusterai2021_clase09_Redes_Neuronales_Clasifacion_PyTorch","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.13 ('PhD')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"4c3760444b418b510111f445cc2c46200fe77d738ac8bb8298f5c5cbe546de8e"}}},"nbformat":4,"nbformat_minor":0}
