{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "__Universidad Tecnológica Nacional, Buenos Aires__<br/>\n",
    "__Ingeniería Industrial__<br/>\n",
    "__Cátedra de Ciencia de Datos - Curso I5521 - Turno sabado mañana__<br/>\n",
    "__Elaborado por: Agustin Velazquez__<br/>\n",
    "__Editado por: Nicolas Aguirre__\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colaboratory\n",
    "\n",
    "En esta notebook presentamos la herramienta **Google Colaboratory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambiente\n",
    "Verificamos si estamos en un ambiente de Colab, y si es asi, montamos nuestro google drive y a la maquina virtual sobre la que esta corriendo la notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si estamos en Colab\n",
    "var_google_colab = 'google.colab' in str(get_ipython())\n",
    "print(var_google_colab)\n",
    "if var_google_colab:\n",
    "  #Montamos nuestro G.Drive\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths\n",
    "Vamos a declarar nuestros paths de imagenes y de archivos segun si estamos o no en Colab.\n",
    "\n",
    "\n",
    "```\n",
    "\"/content/gdrive/My Drive/.......\"\n",
    "\n",
    "```\n",
    "A partir de esta direccion es donde estara nuestra informacion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if var_google_colab:\n",
    "  # Direccion root donde está la jupyter-notebook\n",
    "  root_path = \"/content/gdrive/My Drive/Colab Notebooks/ClusterAI/clusterai_2022/clase03/\"\n",
    "  # Direccion donde guardaremos las imagenes\n",
    "  plot_path = root_path\n",
    "else:\n",
    "  # Si, no estamos en google colab, es que estamos corriendo la \n",
    "  # en local.\n",
    "  root_path = \"\"\n",
    "  plot_path = root_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consultas ?**\n",
    "\n",
    "-------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENm61LzhGGYE"
   },
   "source": [
    "# Iris Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IJZOHXIGGYG"
   },
   "source": [
    "Cuantificar la variación morfológica del Iris con las flores de tres especies relacionadas.\n",
    "\n",
    "El conjunto de datos contiene 50 muestras de cada una de tres especies de Iris (Iris setosa, Iris virginica e Iris versicolor). Se midió cuatro rasgos de cada muestra: lo largo y lo ancho del sépalos y pétalos, en centímetros. Basado en la combinación de estos cuatro rasgos.\n",
    "\n",
    "https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos\n",
    "\n",
    "Los datos son:\n",
    "\n",
    "| Columna | Descripcion |\n",
    "| --- | --- |\n",
    "| ID | Unique ID |\n",
    "| SepalLengthCm | Length of the sepal (cm) |\n",
    "| SepalWidthCm | Width of the sepal (cm) |\n",
    "| PetalLengthCm | Length of the petal (cm) |\n",
    "| PetalWidthCm | Width of the petal (cm) |\n",
    "| Species | name |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo: Clasificar las tres especies de flores segun los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bUZB29cGGYG"
   },
   "outputs": [],
   "source": [
    "# Importamos algunas de las librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset\n",
    "col_names = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','Species']\n",
    "iris_df = pd.read_csv(root_path+'iris.data',names=col_names)\n",
    "# Observamos una parte de los datos\n",
    "iris_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYjLRGP3GGYS",
    "outputId": "112c25ff-0d38-4f9f-e2ae-c60bc2d6f5d2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nuestro objetivo es Species, entonces vemos cuanto tenemos\n",
    "print(\"Species\")\n",
    "print(iris_df['Species'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplot: \n",
    "\n",
    "**Con un pairplot podemos visualizar facilmente similitudes y diferencias entre especies utilizando dos caracteristicas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWLK5a4IGGYX",
    "outputId": "82fa1ebe-2ff2-4071-f22e-2de00923ed2f"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data=iris_df, hue='Species')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizando Pairplot\n",
    "\n",
    "    1. Cual es el significado de los colores?\n",
    "\n",
    "    2. Que datos podemos extraer de la primer columna del pairplot?\n",
    "    \n",
    "    3. Que otras conclusiones se pueden extraer del resto del pairplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s7cVfGoZGGYc",
    "outputId": "fb67e7ed-54ad-4e34-be8b-2f3c174f5016"
   },
   "outputs": [],
   "source": [
    "# Un scatter plot para visualizar dos variables\n",
    "fig, (ax0) = plt.subplots(1,1,figsize=(5,5))\n",
    "g=sns.scatterplot(x=\"SepalLengthCm\", y=\"SepalWidthCm\", data=iris_df, hue = 'Species', \n",
    "                  linewidth=0, alpha = 0.8,ax=ax0)\n",
    "\n",
    "# Veamos como guardar una imagen,\n",
    "plot_name = \"teste_img\"\n",
    "plot_save = str(plot_path + plot_name)\n",
    "plt.savefig(plot_save+\".pdf\",dpi=300)\n",
    "plt.savefig(plot_save+\".png\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C4yZgzLGGGYh"
   },
   "source": [
    "## Multi-Class Problem:\n",
    "\n",
    "**Hay tres tipos de Especies $\\rightarrow$ Multi class problem!**\n",
    "\n",
    "La estrategia que vamos a utilizar es: One vs One "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m46Bj69rGGYi"
   },
   "source": [
    "![Axis diagram](https://image.slidesharecdn.com/linearmodelsandmulticlassclassification2-170312171304/95/linear-models-and-multiclass-classification-25-638.jpg?cb=1489338888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFahHDPrGGYj"
   },
   "source": [
    "\n",
    "La idea es desarrollar un clasificador binario para cada posible par de clases y luego tener un clasificador \"ensamblado\"\n",
    "\n",
    "Si un problema es multi-class (n clases) el One vs One va a construir $\\frac{n(n-1)}{2}$ modelos. Luego la label va a ser finalizada por **el voto de la mayoria**. \n",
    "\n",
    "Si tenemos tres clases, A, B and C. El OVO va a estar compuesto de  $3 = \\frac{3(2-1)}{2}$ clasificadores binarios. \n",
    "El primero va a clasificar A de B, el segundo A de C, y el tercer B de\n",
    "C.\n",
    "\n",
    "Finalmente la muestra $x_i$ va a tener los siguientes labels, e.j. (A, B, B). y por votacion es asignado la clase perteneciente.\n",
    "\n",
    "* Ventajas\n",
    "\n",
    "* Desventajas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de columnas\n",
    "column_names = iris_df.columns.values\n",
    "# Columna target\n",
    "targets_name = ['Species']\n",
    "# Nombres de columnas que no estan en el target\n",
    "features_names = [i_c for i_c in column_names if i_c not in targets_name]\n",
    "print('Features:',features_names)\n",
    "print('Target:',targets_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhuPMAJUGGYk",
    "outputId": "a8dc69c0-ba2e-43ac-eed4-becb03a049e9"
   },
   "outputs": [],
   "source": [
    "# 1) Separamos lo que son las features del target.\n",
    "features = iris_df[features_names]           \n",
    "target = iris_df[targets_name]\n",
    "print(features)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos feature matrix en \"X\"\n",
    "X = features.values\n",
    "# Guardamos target vector in \"Y\"\n",
    "Y = target.values                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(Y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder\n",
    "**LabelEncoder recibe valores (array o columnas de Pandas DataFrame) y le asigna a cada valor unico una clase.** \n",
    "\n",
    "**Luego nos devuelve los valores \"codificados\" entre 0 y n_clases - 1.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0bIqC0WqGGYo",
    "outputId": "bf4d2c81-e637-4db7-fc62-638cb8b754ab",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Transformamos el label del target en formato numerico para poder procesarlo\n",
    "\n",
    "# Importamos preprocessing de la libreria scikit-learn (a.k.a. sklearn)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 1 - Primero creo un objeto (con determinados argumentos) que llama a la funcion que deseo\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# 2 - AL objeto le aplico la funcion fit a los datos desados \n",
    "Y = le.fit_transform(Y.ravel()) #.ravel() para evitar el warning!\n",
    "\n",
    "# 3 - Agregamos la nueva columna a nuestro DataFrame\n",
    "iris_df['EncodedSpecies'] = Y\n",
    "\n",
    "print('Classes:',le.classes_)\n",
    "print('Response variable after encoding:',Y)\n",
    "iris_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56d52FVoGGYt",
    "outputId": "3dfaf15e-4708-4905-e169-053693339f8b"
   },
   "outputs": [],
   "source": [
    "# Separar train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=4)\n",
    "#random_state: seed utilizada para poder replicar el experimento ya que la funcion tiene\n",
    "#componentes aleatorios\n",
    "#test_size: numero entre 0 y 1 e indicar la proporcion del dataset que va a ser test. \n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06eo-xgVGGY0",
    "outputId": "96e46ec6-4766-47f4-c36b-c25aa8f4d592"
   },
   "outputs": [],
   "source": [
    "# Auto scaling train-set (mean = 0, std = 1)\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "# Los valores de media y var quedan guardados en el scaler.\n",
    "print(scaler.mean_)\n",
    "print(scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3gaxIx5GGY5",
    "outputId": "9b406fb7-a33c-4a1a-b35a-d3bbb139411a"
   },
   "outputs": [],
   "source": [
    "# Transformamos los datos de x_train ...\n",
    "x_train_scaled = scaler.transform(x_train)  \n",
    "print(x_train_scaled.mean(axis=0))\n",
    "print(x_train_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora vamos a estandarizar los datos de test que una vez aplicado no deben tocarse hasta la validacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled = scaler.transform(x_test)\n",
    "print(x_test_scaled.mean(axis=0))\n",
    "print(x_test_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repasemos...**\n",
    "\n",
    "- **Cual es el objetivo de escalar los datos?**\n",
    "\n",
    "\n",
    "- **Por que no escalamos y_test?**\n",
    "\n",
    "\n",
    "- **Por que $\\mu_i \\neq 0 $ y $\\sigma_i \\neq 1$ en *x_test_scaled*?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "**Vamos a ver como se utilizan SVM en su forma de clasificador (SVC) con la libreria sklearn.**\n",
    "\n",
    "**Previamente, crearemos un 'objeto' GridSearchCV (Grid Search Cross-Validation) al cual le pasaremos un diccionario de python cuyas keys seran los hyperparametros, y sus values, seran los distintos valores que querramos evaluar.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-CMn07WGGZD",
    "outputId": "3f925273-c6f3-4aea-bee5-c25bfd3e07e7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Creamos el modelo Support Vector Classification (SVC)\n",
    "svc = svm.SVC()\n",
    "\n",
    "# Hyperparametros\n",
    "parameters = {'kernel':('linear', 'rbf'),\n",
    "              'C':[1, 10, 100, 1000],\n",
    "              'gamma':[0.0001,0.001, 0.01, 0.1,1, 10,100]}\n",
    "# \n",
    "clf = GridSearchCV(svc, # modelo\n",
    "                   param_grid = parameters, # Hyperparametros\n",
    "                   refit = True, # refit nos devuelve el modelo con los mejores parametros encontrados \n",
    "                   cv = 5, # cv indica la cantidad de folds\n",
    "                   verbose=1)\n",
    "clf.fit(x_train_scaled, y_train.ravel())\n",
    "# Non-linear SVM\n",
    "#model = SVC(C=1.0, kernel='rbf', gamma = 'auto' )\n",
    "#model.fit(x_train_scaled, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Que quiere decir el mensaje:** <br/><br/>\n",
    "*'Fitting 5 folds for each of 56 candidates, totalling 280 fits' ?, De donde sale 56 y 280?*<br/><br/>\n",
    "**Veamos que sucede si aumentamos verbose ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Toda la informacion del entrenamiento esta en cv_results_\n",
    "scores = clf.cv_results_\n",
    "#Veamosla ...\n",
    "scores_df = pd.DataFrame.from_dict(scores)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = \"CV_results.csv\"\n",
    "csv_save = str(root_path + \"CV_results.csv\")\n",
    "scores_df.to_csv(csv_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMVY01AlGGZG"
   },
   "source": [
    "**Que es lo que hizo internamente GridSearchCV?**\n",
    " \n",
    "![Grid_SearchCV](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "[**Fuente**](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habiamos dicho que la funcion de mapeo $\\phi(x)$ la utilizariamos para \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\langle\\phi(x_i), \\phi(x_j)\\rangle& = \n",
    "\\begin{pmatrix}\n",
    "\\langle\\phi(x_1), \\phi(x_1)\\rangle& \\langle\\phi(x_1), \\phi(x_2)\\rangle & \\cdots & \\langle\\phi(x_1), \\phi(x_n)\\rangle\\\\\n",
    "\\langle\\phi(x_2), \\phi(x_1)\\rangle& \\langle\\phi(x_2), \\phi(x_2)\\rangle & \\cdots & \\langle\\phi(x_2),\\phi(x_n)\\rangle\\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "\\langle\\phi(x_n), \\phi(x_2)\\rangle& \\langle\\phi(x_n), \\phi(x_2)\\rangle & \\cdots & \\langle\\phi(x_n), \\phi(x_n)\\rangle\\\\\n",
    "\\end{pmatrix} \\\\\n",
    "& \\\\\n",
    "\\text{Donde para el}\\\\\n",
    "\\text{Kernel Gaussiano:}\\\\\n",
    "\\\\\n",
    "\\langle\\phi(x_i), \\phi(x_j)\\rangle  & = K (x_i, x_j)\\\\\n",
    "& =   \\exp(-\\frac{\\lVert x_i-x_j\\rVert^2}{2\\sigma^2})\\\\\n",
    "& = \\exp(-\\gamma \\lVert x_i-x_j\\rVert^2)  \\qquad \\text{ donde } \\gamma = \\frac{1}{2\\sigma^2}\\\\\n",
    "& = \\exp(-\\gamma (x_{i}^{2}+x_{j}^{2}-2 x_{i} x_{j}^T)\\\\\n",
    "& = \\exp(-\\gamma (x_{i}x_{i}^T+x_{j}x_{j}^T-2 x_{i} x_{j}^T)\\\\\n",
    "\\text{Entonces, gracias al 'kernel trick'}\\\\\n",
    "\\\\\n",
    "\\langle\\phi(x_i), \\phi(x_j)\\rangle \n",
    "& = \\begin{pmatrix}\n",
    "K (x_1, x_1)& K (x_1, x_2) & \\cdots & K (x_1, x_n)\\\\\n",
    "K (x_2, x_1)& K (x_2, x_2)& \\cdots & K (x_2, x_n)\\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "K (x_n, x_1)& K (x_n, x_2)& \\cdots & K (x_n, x_n)\\\\\n",
    "\\end{pmatrix}\\\\\n",
    "\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Veamos una forma vectorial de como calcularla ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Norma PARA CADA X_i\n",
    "x_norm = np.sum(x_train_scaled ** 2, axis = -1)\n",
    "# Un valor de gamma random ...\n",
    "gamma = 0.4\n",
    "# Por ultimo aplicamos la funcion exponencial a cada elemento de la matriz nxn\n",
    "K = np.exp(- gamma * (x_norm[:,None] + #(n,1)\n",
    "                      x_norm[None,:] - #(1,n) --->(n,n)\n",
    "                      2 * np.dot(x_train_scaled, x_train_scaled.T) # (n,n)\n",
    "                     ))\n",
    "# Veamoslo en un heatmap ...\n",
    "ax = sns.heatmap(K,\n",
    "            yticklabels=False,\n",
    "            xticklabels=False)\n",
    "ax.set_title('Kernel Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Veamos como hacerlo utilizando la libreria sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "# Gaussian kernel/RBF kernel\n",
    "ax = sns.heatmap(rbf_kernel(x_train_scaled, gamma=0.4),\n",
    "               yticklabels=False,\n",
    "               xticklabels=False)\n",
    "ax.set_title('Kernel Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas:**\n",
    "1. **Que representa el color ?**\n",
    "    * Rta:\n",
    "\n",
    "2. **Que es y por que tiene ese valor la diagonal ?**\n",
    "    * Rta:\n",
    "    \n",
    "3. **Podemos observar algun tipo de patron ?**\n",
    "    * Rta:\n",
    "    \n",
    "    * Hint: *np.argsort(y_train)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "**Utilizando predict utilizamos la funcion ya entrenada previamente para ajustarla a los datos de test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0EvZhOWUGGZH"
   },
   "outputs": [],
   "source": [
    "# Prediction sobre las muestras de test\n",
    "y_pred = clf.predict(x_test_scaled)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Con score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3G-TgNKmGGZK"
   },
   "outputs": [],
   "source": [
    "# Model prediciton\n",
    "print(clf.score(x_test_scaled,y_test.ravel())) #obtenemos el promedio de la accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeTVheCWGGZM"
   },
   "source": [
    "## Confusion matrix\n",
    "\n",
    "**Es un herramienta que permite la visualización del desempeño de un algoritmo que se emplea en aprendizaje supervisado.** \n",
    "\n",
    "**Cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a las instancias en la clase real.**\n",
    "\n",
    "**Uno de los beneficios de las matrices de confusión es que facilitan ver si el sistema está classificando erroneamente las clases.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qfTJbRexGGZO"
   },
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EoRevahWGGZS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df_cm = pd.DataFrame(cm, #Valores\n",
    "                     index = [i for i in np.unique(Y)], # rows\n",
    "                     columns = [i for i in np.unique(Y)]) # Column\n",
    "plt.figure(figsize = (8,6))\n",
    "ax = sns.heatmap(df_cm, annot=True, cmap=\"viridis\")\n",
    "plt.yticks(rotation=0)\n",
    "ax.xaxis.tick_top() # x axis on top\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1.  Dividimos el dataset en features y target\n",
    "    \n",
    "    2.  Aplicamos un encoder a la variable objetivo\n",
    "    \n",
    "    3.  Separamos nuestro data-set en train y test set\n",
    "    \n",
    "    4.  Estandarizamos el set de entrenamiento y sobre lo resultados el set de test. \n",
    "    \n",
    "    5.  Aplicamos un cross validation grid search con kernel linea y gaussiano y diversos parametros sobre el train set\n",
    "    \n",
    "    6.  Realizamos la prediccion sobre el test set y obtenemos el score\n",
    "    \n",
    "    7.  Realizamos una Matriz de Confusion para obtener una mejor vision del error obtenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASeYg1MnGGZW"
   },
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C7fqPu3bGGZY"
   },
   "source": [
    "Diferencia entre otras funciones con SVM\n",
    "\n",
    "En el siguiente link https://scikit-learn.org/stable/modules/svm.html#svm-classification encontramos diferentes funciones utilizadas en Sklearn sobre el mismo dataset y los resultados de la clasificacion. \n",
    "\n",
    "![Axis diagram](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_001.png)\n",
    "[**Fuente**](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consultas ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Meme](https://editor.analyticsvidhya.com/uploads/30927SVM%201.png)\n",
    "[**Fuente**](https://editor.analyticsvidhya.com/uploads/30927SVM%201.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cluster_clase03_fit_svm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
