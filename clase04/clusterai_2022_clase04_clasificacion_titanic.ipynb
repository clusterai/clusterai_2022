{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "__Universidad Tecnológica Nacional, Buenos Aires__<br/>\n",
    "__Ingeniería Industrial__<br/>\n",
    "__Cátedra de Ciencia de Datos - Curso I5521 - Turno sabado mañana__<br/>\n",
    "__Elaborado por: Santiago Chas__<br/>\n",
    "__Editado por: Nicolas Aguirre__\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colaboratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos si estamos en Colab\n",
    "var_google_colab = 'google.colab' in str(get_ipython())\n",
    "print(var_google_colab)\n",
    "if var_google_colab:\n",
    "  #Montamos nuestro G.Drive\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/gdrive',force_remount=True)\n",
    "  # Direccion root donde está la jupyter-notebook\n",
    "  root_path = \"/content/gdrive/My Drive/Colab Notebooks/ClusterAI/clusterai_2022/clase04/\"\n",
    "  # Direccion donde guardaremos las imagenes\n",
    "  plot_path = root_path\n",
    "else:\n",
    "  # Si, no estamos en google colab, es que estamos corriendo la \n",
    "  # en local.\n",
    "  root_path = \"\"\n",
    "  plot_path = root_path  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ENm61LzhGGYE"
   },
   "source": [
    "# Titanic Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción\n",
    "|Variable |\tDefinition |\tKey |\n",
    "| --- | --- | --- |\n",
    "|survival|\tSurvival|\t0 = No, 1 = Yes|\n",
    "|pclass|\tTicket class|\t1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|sex|\tSex\t| |\n",
    "|Age|\tAge in years| |\t\n",
    "|sibsp|\t# of siblings / spouses aboard the Titanic| |\t\n",
    "|parch|\t# of parents / children aboard the Titanic| |\t\n",
    "|ticket|\tTicket number | |\t\n",
    "|fare|\tPassenger fare |\t|\n",
    "|cabin|\tCabin number\t| |\n",
    "|embarked|\tPort of Embarkation\t|C = Cherbourg, Q = Queenstown, S = Southampton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo: Clasificar las tres especies de flores segun los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1bUZB29cGGYG"
   },
   "outputs": [],
   "source": [
    "# Importamos algunas de las librerias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el dataset\n",
    "titanic_df = pd.read_csv(root_path+\"train.csv\")\n",
    "# Observamos una parte de los datos\n",
    "titanic_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos columnas que no nos interesan\n",
    "titanic_df = titanic_df.drop(['PassengerId',\"Name\", \"Ticket\",\"Cabin\"],axis=1)\n",
    "titanic_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = titanic_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (titanic_df.isnull().sum()/titanic_df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a = ['S','Q','C']\n",
    "titanic_df.loc[~titanic_df['Embarked'].isin(list_a),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lleno vacíos de age con edad promedio\n",
    "#titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace = True)\n",
    "# Lleno Embarked vacíos con \"S\"\n",
    "titanic_df['Embarked'].fillna('S', inplace = True)\n",
    "# chequeamos que todo esta con los NaN\n",
    "total = titanic_df.isnull().sum().sort_values(ascending=False)\n",
    "percent = (titanic_df.isnull().sum()/titanic_df.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "# EDA\n",
    "\n",
    "**Tarea**:\n",
    "\n",
    "* Imprimir la matriz de correlación\n",
    "\n",
    "* Histograma y Boxplot por Edad segun \"Supervivencia\"\n",
    "\n",
    "* Boxplot para ver Edad en función de la clase (Pclass).\n",
    "\n",
    "\n",
    "-----------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocessing & Utils.\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#Metricas\n",
    "from sklearn.metrics import accuracy_score,roc_curve, auc,confusion_matrix\n",
    "# Modelos\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las variables de entrenamiento y objetivo.\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Embarked']\n",
    "target = ['Survived']\n",
    "# Generamos X e Y\n",
    "X = titanic_df.loc[:,features]\n",
    "Y = titanic_df.loc[:,target]\n",
    "\n",
    "# Spliteamos Train y test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline y ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a continuar con la implementacion del workflow ede trabajo de ML incorporando dos herramientas.\n",
    "\n",
    "* [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)\n",
    "\n",
    "\n",
    "Pipeline aplica secuencialmente una lista de transformaciones y un estimador final. \n",
    "\n",
    "Los pasos (*steps*) intermedios de la cadena deben ser \"transformadores\", es decir, deben implementar los métodos **.fit()** y **transform**. \n",
    "\n",
    "* [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n",
    "\n",
    "ColumnTransformer aplica transformadores a las columnas de un array o DataFrame de pandas.\n",
    "\n",
    "Este estimador permite transformar diferentes columnas o subconjuntos de columnas de la entrada por separado y las características generadas por cada transformador se concatenarán para formar un único espacio de características. \n",
    "\n",
    "Esto es útil para datos heterogéneos o columnares, para combinar varios mecanismos de extracción de características o transformaciones en un solo transformador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferenciemos los tipos de variables (numericas, categoricas)\n",
    "varaibles_num_1 = [\"Age\", \"Fare\"]\n",
    "varaibles_num_2 = [\"SibSp\",\"Parch\"]\n",
    "variables_cat = [\"Embarked\", \"Sex\", \"Pclass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definamos las transformaciones para cada tipo de variable:\n",
    "transformacion_num_1 = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "transformacion_num_2 = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())]\n",
    ")\n",
    "\n",
    "transformacion_cat = OneHotEncoder(handle_unknown=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntemos todo:\n",
    "preprocesamiento = ColumnTransformer(\n",
    "    # (nombre,transformacion, columnas )\n",
    "    transformers=[ \n",
    "        (\"num1\", transformacion_num_1, varaibles_num_1),\n",
    "        (\"num2\", transformacion_num_2, varaibles_num_2),\n",
    "        (\"cat\", transformacion_cat, variables_cat),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Veamos que nos devuelve hasta aca el preprocesamiento ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocesamiento.fit_transform(X_train), columns=preprocesamiento.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien, ya tenemos el preprocesamiento consolidado ...\n",
    "\n",
    "Utilicemos nuevamente **Pipeline** para agregarle un estimador ... por ejemplo ... **LogisticRegression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps=[(\"preprocesamiento\", preprocesamiento), (\"estimador\", LogisticRegression())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow hasta el momento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREGUNTAS ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos y sus Hyperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sabemos podemos querer comparar muchos modelos ...\n",
    "\n",
    "Por ejemplo, dado un modelo de SVC hariamos ....\n",
    "\n",
    "```\n",
    "#Modelo\n",
    "svc = SVC()\n",
    "#Pipeline\n",
    "pipeline = Pipeline(\n",
    "    steps=[(\"preprocesamiento\", preprocesamiento), (\"estimador\", svc)]\n",
    "    )\n",
    "\n",
    "# Hyperparametros\n",
    "parameters = {'estimador__kernel':('linear', 'rbf'),\n",
    "              'estimador__C':[1, 10, 100],\n",
    "              'estimador__gamma':[0.001, 0.1,1, 10]}\n",
    "# GridhSearchCV\n",
    "clf = GridSearchCV(pipeline, # pipeline\n",
    "                   param_grid = parameters, # Hyperparametros\n",
    "                   refit = True, # refit nos devuelve el modelo con los mejores parametros encontrados \n",
    "                   cv = 5, # cv indica la cantidad de folds\n",
    "                   verbose=1)\n",
    "#Fit\n",
    "clf.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "```\n",
    "\n",
    "Y para un modelo distinto, tendriamso que definir un nuevo modelo, pasarselo a Pipiline ...etc, etc, etc ...\n",
    "\n",
    "![blablabla](https://i.giphy.com/media/bTvrXKt7qoALMMklz4/giphy.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pero lo que queremos es poder compactar **todo**:\n",
    "\n",
    "* Pipeline\n",
    "\n",
    "* ColumnTransformer\n",
    "\n",
    "* Models\n",
    "\n",
    "* GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya tenemos CASI todo ... solo nos falta definir los modelos y sus hyperparametros\n",
    "\n",
    "# ATENCION: \"__\" (son dos \"_\" seguidos) se usa como indicador de features del estimador. \n",
    "\n",
    "parametros = [\n",
    "    {\n",
    "        \"estimador\": (LogisticRegression(),),\n",
    "          \"estimador__C\": (0.001,1,10)\n",
    "    }, \n",
    "    {\n",
    "        \"estimador\": (RandomForestClassifier(),),\n",
    "        \"estimador__n_estimators\": [50, 100], \n",
    "        \"estimador__max_features\": [3, 8], \n",
    "        \"estimador__max_depth\": [5, 20, 50], \n",
    "        \"estimador__min_samples_leaf\":[ 8, 10]\n",
    "    },\n",
    "    {\n",
    "        \"estimador\": (SVC(),),\n",
    "          \"estimador__kernel\":('linear', 'rbf'), \n",
    "          \"estimador__C\":(1, 10, 100), \n",
    "          \"estimador__gamma\":(0.001, 0.01, 0.1,1, 10)          \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el GSCV: \n",
    "grid_search = GridSearchCV(pipeline, parametros,\n",
    "                  refit = True, # refit nos devuelve el modelo con los mejores parametros encontrados \n",
    "                   cv = 5, # cv indica la cantidad de folds\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, Y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toda la informacion del entrenamiento esta en cv_results_\n",
    "scores = grid_search.cv_results_\n",
    "#Veamosla ...\n",
    "scores_df = pd.DataFrame.from_dict(scores)\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best parameters are %s with a score of %0.2f\" % (grid_search.best_params_, grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(preprocesamiento.transform(X_test), columns=preprocesamiento.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction sobre las muestras de test\n",
    "Y_pred = grid_search.predict(X_test)\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computo el accuracy (comparar ytest vs ypred)\n",
    "test_acc = accuracy_score(Y_test, Y_pred)\n",
    "print(\"El accuracy es \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and plot AUC\n",
    "Y_proba = grid_search.predict_proba(X_test)\n",
    "fpr1, tpr1, thresholds = roc_curve(Y_test.astype('int'), Y_proba[:,1], drop_intermediate = False)\n",
    "auc_value = auc(fpr1, tpr1)\n",
    "print(\"El AUC es = \" + str(auc_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr1, tpr1, lw=2, alpha=0.7 , label = 'ROC curve', color = 'b')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r', alpha=.8)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(False)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Confusion Matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "df_cm = pd.DataFrame(cm, index = ['No sobrevivió', 'Sobrevivió'], columns = ['No sobrevivió', \"Sobreviviente\"])\n",
    "plt.figure(figsize = (6,4))\n",
    "sns.heatmap(df_cm, annot=True,fmt='g')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREGUNTAS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme](https://pythonprogramming.net/static/images/svm/machineLearning.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "cluster_clase03_fit_svm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('PhD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4c3760444b418b510111f445cc2c46200fe77d738ac8bb8298f5c5cbe546de8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
