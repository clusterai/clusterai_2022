{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"stl-KuqP0rLJ"},"source":["# Installs en Google Colab."]},{"cell_type":"code","metadata":{"id":"EqjH_TEKqlCY"},"source":["# Instalamos librerias\n","!pip install datasets\n","!pip install tokenizers\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkH3Wl131O21"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"Y4p-chnUy2SP"},"source":["# Pytorch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, Dataset, DataLoader\n","from torch.optim import Adam\n","import torch.nn.functional as F\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(torch.__version__)\n","print(device)\n","# Datasets\n","import datasets\n","from datasets import load_dataset\n","#Tokenizer\n","from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n","# Varias\n","import os,sys,humanize,psutil,GPUtil,gc\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","# Metricas\n","from sklearn.metrics import f1_score, classification_report,confusion_matrix,accuracy_score\n","from scipy.special import softmax\n","# Barra de Progreso\n","from tqdm.autonotebook import tqdm , trange"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aZR16dgHHyfp"},"source":["# Fucion para printear el uso RAM / GPU \n","def mem_report():\n","  print(\"CPU RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ))\n","  GPUs = GPUtil.getGPUs()\n","  for i, gpu in enumerate(GPUs):\n","    print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHADV0Ea2Fl0"},"source":["# Para ver el reporte de la GPU que nos toco y la cantidad de RAM\n","torch.cuda.empty_cache()\n","gc.collect()\n","mem_report()\n","!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6So4hgcdldk7"},"source":["# Montamos el Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G0JuwkKeH9dW"},"source":["---\n","\n","# ClusterAI 2022\n","\n","# Ciencia de Datos - Ingeniería Industrial - UTN BA\n","\n","# clase_11: Practica Redes Neuronales Recurrentes - NLP - Sentiment Analisys\n","\n","### Elaborado por: Aguirre Nicolas\n","\n","Imagenes: https://guide.allennlp.org/"]},{"cell_type":"markdown","metadata":{"id":"oq-sk09JHScL"},"source":["---\n","\n","El objetivo de esta notebook es clasificar el *sentimiento* (Positivo, Negativo o Neutral) de un dataset de criticas/reviews sobre aplicaciones del Google Play Store.\n","\n","Para lograr ésto, nuestro modelo tiene que ser capaz de \"interpretar\" el contenido de un texto, el cual puede ser descompuesto en:\n","\n","  * Letras / Caracteres\n","  * Palabras \n","  * Frases, \n","  * etc.\n","\n","Esto representa un primer desafio: como y cuales seran los inputs ($X_t$) que ingrese al modelo?. Este punto es el que discutiremos con la tecnica llamada *Tokenization*. \n","\n","El problema se vuelve mas complejo cuando la *temporalidad* o el *orden* de las de los inputs (el $t$ de $X_t$) define el significado de una frase:\n","\n","  * Un texto con todas sus palabras mezcladas carece de sentido, verdad? o,\n","  * Una negacion antes o despues de una frase puede cambiar completamente el significado de la misma (Ej: \"*no recomiendo descargar esta app*\" vs. \"*recomiendo descargar esta app, no pierdan la oportunidad*\"). \n","  \n","Esta dependencia temporal se abordará (en este ejemplo en particular) con el uso de Redes Neuronales Recurrentes (**RNN**, de sus siglas en ingles.) \n","\n","# 1) Objetivo\n","\n","![](https://guide.allennlp.org/part1/your-first-model/designing-a-model-1.svg)\n","## 2) Tokenization\n","![](https://guide.allennlp.org/part1/your-first-model/designing-a-model-2.svg)\n","## 3) Arquitectura\n","![](https://guide.allennlp.org/part1/your-first-model/designing-a-model-4.svg)\n","## 4) Entrenamiento\n","![](https://guide.allennlp.org/part1/your-first-model/designing-a-model-5.svg)"]},{"cell_type":"markdown","metadata":{"id":"x1HEmDaT0yBx"},"source":["# Dataset\n","\n","Google Play Store:\n","https://www.kaggle.com/lava18/google-play-store-apps\n","\n","Archivo: **googleplaystore_user_reviews.csv**\n","\n"]},{"cell_type":"code","metadata":{"id":"znGMiMLxIc51"},"source":["root_path = \"/content/gdrive/MyDrive/Colab Notebooks/ClusterAI/clusterai_2022/clase11/\"\n","google_df = pd.read_csv(root_path+\"googleplaystore_user_reviews.csv\")\n","google_df.dropna(how ='any', inplace = True)\n","google_df = google_df.drop_duplicates()\n","google_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hX7EyabNIc3b"},"source":["# Vamos a eliminar columnas innecesarias y renombrar las que nos sirven.\n","google_df = google_df.drop(columns=['Sentiment_Polarity','App','Sentiment_Subjectivity']).rename(columns={\"Translated_Review\": \"text\", \"Sentiment\": \"label\"})\n","google_df['text'] = google_df['text'].astype(str)\n","google_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6LVtKPkFI6_N"},"source":["## Label Encoder"]},{"cell_type":"code","metadata":{"id":"M4UC3wwWIcwz"},"source":["# Pasemos a codigo el label de cada sample\n","le = preprocessing.LabelEncoder()\n","google_df['label'] = le.fit_transform(google_df['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKZTfDZfIctt"},"source":["name_classes = []\n","for i_c in range(len(le.classes_)):\n","    cls_name = le.inverse_transform([i_c]) \n","    print(i_c,cls_name[0])\n","    name_classes.append(cls_name[0])\n","name_classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7KVJ_2r6PVi8"},"source":["## Split"]},{"cell_type":"code","metadata":{"id":"H2NBevAaIcq9"},"source":["google_df_train,google_df_test = train_test_split(google_df,random_state=0,test_size=0.15)\n","google_df_train,google_df_val = train_test_split(google_df_train,random_state=0,test_size=0.15)\n","\n","google_df_train.reset_index(drop=True,inplace=True)\n","google_df_val.reset_index(drop=True,inplace=True)\n","google_df_test.reset_index(drop=True,inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g5fl29MRJJ-F"},"source":["To CSV"]},{"cell_type":"code","metadata":{"id":"6S4mypE7JMUK"},"source":["google_df_train.to_csv(root_path+\"train.csv\",index=False)\n","google_df_val.to_csv(root_path+\"val.csv\",index=False)\n","google_df_test.to_csv(root_path+\"test.csv\",index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mbkg2DuMJVtE"},"source":["# 2) Tokenizacion"]},{"cell_type":"markdown","metadata":{"id":"PKoXrA0TJZvj"},"source":["## Loading Dataset"]},{"cell_type":"code","metadata":{"id":"awmqe8UHJMNj"},"source":["# Definiremos las features de nuestro objeto \"dataset\" \n","# de la libreria llamada \"Dataset\" de HuggingFace (libreria especializada en NLP)\n","features_ds = datasets.Features(\n","        {\"text\": datasets.Value(\"string\"),\n","         \"label\": datasets.features.ClassLabel(names=name_classes)}\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oy3LL78uJMLe"},"source":["# Cargamos los CSV en el objeto dataset (train, val, y test set).\n","# Tambien le pasamos las features que definimos en la celda anterior\n","dataset_google = load_dataset('csv', data_files={'train':root_path+\"train.csv\",\n","                                                 'val':root_path+\"val.csv\",\n","                                          'test':root_path+\"test.csv\"},\n","                             features=features_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Lv7NZ7OJmZD"},"source":["## Algoritmo/modelo de Tokenizacion"]},{"cell_type":"code","metadata":{"id":"XIHFMnLRJMH9"},"source":["# Primero definimos el Algoritmo /modelo que vamos a utilizar\n","# En este caso particular, vamos a usar el \"wordLevel\". Vamos a separar el texto por espacios y signos de puntuacion.\n","\n","# Debido a que luego definiremos una cantidad maxima de palabras que tendremos \n","# en nuestro vocabulario, tenemos que definir tambien un \"token\" especial para \n","# las palabras que NO queden incorporadas.\n","unk_token = \"[UNK]\"\n","\n","# Creamos nuestro objeto Tokenizer, el cual le iremos incorporando funciones\n","# en las siguientes celdas\n","tokenizer_custom = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HS0BylWpJqQi"},"source":["## Pre-Tokenizer\n","\n","La pretokenización es el acto de dividir un texto en objetos más pequeños "]},{"cell_type":"code","metadata":{"id":"drPhQRgRJMFP"},"source":["# Pre-tokenizador:\n","pre_tok = pre_tokenizers.Whitespace()\n","tokenizer_custom.pre_tokenizer = pre_tok"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4Js-uJNJt9s"},"source":["## Trainer"]},{"cell_type":"code","metadata":{"id":"1AW9X6UeJMC5"},"source":["# Importante: \n","# 1) Cada Tokenizer tiene su trainer asociado ya que internamiente son algoritmos distintos\n","# 2) Al trainer le debemos pasar tambien los tokens especiales que utilicemos \n","\n","token_trainer = tokenizer_custom.model.get_trainer()\n","# Definimos el tamaño de nuestro vocabulario\n","token_trainer.vocab_size = 15000\n","# La cantidad minima de veces que una palabra/token tiene que aparecer para ser incorpoada\n","token_trainer.min_frequency = 2\n","# Definimos los tokens especiales que utilizaremos.\n","special_tokens = [\"[UNK]\",\"[PAD]\",\"[SOS]\",\"[EOS]\"]\n","token_trainer.special_tokens = special_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DIN57pqHJy2z"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"0tYFupcUJ1Qd"},"source":["# Creamos una funcion que itere por batch en nuestro dataset['text']\n","def batch_iterator(dataset,batch_size):\n","    for i in range(0, len(dataset), batch_size):\n","        yield dataset[i : i + batch_size][\"text\"]\n","\n","# Stackoverflow - yield\n","# https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKR0dWT5J1Nd"},"source":["# Nuestor tokenizer ya esta en condiciones de ser entrenado y definir que indices \n","# se le asignara a cada palabra/token\n","\n","batch_size_tok = 1000 # Cantidad de frases que vamos muestreando\n","tokenizer_custom.train_from_iterator(batch_iterator(dataset_google['train'],batch_size_tok), trainer=token_trainer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B5C7ImilJ8Oa"},"source":["## Post- Tokenization"]},{"cell_type":"markdown","metadata":{"id":"McpluEARJ_G1"},"source":["### [SOS] & [EOS]\n","\n","Start of Sentence & End Of Sentence"]},{"cell_type":"code","metadata":{"id":"zbKcqKCOJ1Kd"},"source":["# Comunmente se incorporan estos tokens para que el modelo aprenda cuando una\n","# frase comienza y cuando una frase termina.\n","post_toke = processors.TemplateProcessing(\n","    single=\"[SOS] $A [EOS]\",\n","    special_tokens=[\n","        (\"[SOS]\", 2),\n","        (\"[EOS]\", 3),\n","    ],\n",")\n","tokenizer_custom.post_processor = post_toke"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnFI_KA1J1IR"},"source":["encoding = tokenizer_custom.encode(\"what is this ?veryrareword test 123 111\")\n","print(encoding.ids)\n","print(encoding.tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DHrzmrqEKObS"},"source":["### Padding y Truncation"]},{"cell_type":"code","metadata":{"id":"eAuC80ZfJ1GG"},"source":["# Para entrenar nuestros modelos en batch, la longitud de todos los textos deben ser iguales.\n","# Por tal motivo, vamos a hablitar \"padding\" y \"truncation\" para los textos mas cortos y largos, respectivamente.\n","\n","# Definimos la longitud maxima del modelo\n","max_lenght = 120\n","\n","# Padding: agregar datos NO-informativos para alcanzar una longitud esperada \n","# en una secuencia.\n","tokenizer_custom.enable_padding(direction='right', # El padding es a la derecha\n","                                pad_id=1 ,\n","                                pad_to_multiple_of = max_lenght,\n","                                pad_token=\"[PAD]\")\n","#Truncation\n","tokenizer_custom.enable_truncation(max_lenght,\n","                                   stride=0,\n","                                   strategy='longest_first')\n","\n","encoding = tokenizer_custom.encode(\"what is this ?veryrareword test 123 111\")\n","print(encoding.ids)\n","print(encoding.tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-VKo-qndKWpi"},"source":["## Dataset Tokenization\n","\n","Recien ahora, con nuestro tokenizador entrenado podremos aplicarlo a nuestro dataset!"]},{"cell_type":"code","metadata":{"id":"BAjpmD2tJ1Dv"},"source":["# Creamos una funcion que aplicara el encode a nuestro todo el dataset y va a crear\n","def _tokenize(examples):\n","    result = {}\n","    for e in tokenizer_custom.encode_batch(examples['text']):\n","        result.setdefault('ids', []).append(e.ids)\n","        result.setdefault('tokens', []).append(e.tokens)\n","        result.setdefault('offsets', []).append(e.offsets)\n","        result.setdefault('special_tokens_mask', []).append(e.special_tokens_mask) # if id is special token or not\n","        result.setdefault('text_lenght', []).append(np.sum(np.array(e.special_tokens_mask)==0)+2)\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0dxE2eMJ1Ay"},"source":["dataset_google = dataset_google.map(_tokenize, batched=True,batch_size=5000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ut45lt-eKeR9"},"source":["## Ejemplo"]},{"cell_type":"code","metadata":{"id":"sqP1kpSiJ0-M"},"source":["idx_rnd = np.random.randint(0,len(dataset_google['train']))\n","idx_rnd = dataset_google['train'][idx_rnd]\n","print(name_classes[idx_rnd['label']],'\\r\\n')\n","print(idx_rnd['text'],'\\r\\n')\n","print(idx_rnd['tokens'],'\\r\\n')\n","print(idx_rnd['ids'],'\\r\\n')\n","print(idx_rnd['special_tokens_mask'],'\\r\\n')\n","print(idx_rnd['label'],'\\r\\n')\n","print(idx_rnd['text_lenght'],'\\r\\n')\n","np.sum(np.array(idx_rnd['special_tokens_mask'])==0)+2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E4_dGt5BKhaa"},"source":["## Guardamos el tokenizador"]},{"cell_type":"code","metadata":{"id":"hGvag66UJ07m"},"source":["tokenizer_custom.save(root_path+\"google_tokenizer.json\",pretty=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qvAuA-1e02-R"},"source":["### Import"]},{"cell_type":"code","metadata":{"id":"gsZPPwUSBdNm"},"source":["\"\"\"tokenizer_custom = Tokenizer.from_file(root_path+\"google_tokenizer.json\")\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-cIirakokvWT"},"source":["Hasta aqui terminamos con la Tokenization ...\n","\n","En el siguiente link podran encontrar los distintos algoritmos de tokenization y la forma de aplicarlos para otros casos de NLP.\n","\n","[Documentacion HuggingFace](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html)\n","\n","**Consultas hasta el momento?** "]},{"cell_type":"markdown","metadata":{"id":"hhu1wUwt1Ev4"},"source":["# Formato a PyTorch"]},{"cell_type":"markdown","metadata":{"id":"YevioYjnawCH"},"source":["A cada uno de los subset los pasamos a formato pytorch, y le indicamos que columnas queremos (opcional indicarle tambien el dispositivo, en este caso, cuda=GPU)"]},{"cell_type":"code","metadata":{"id":"DZlVaK_7CumD"},"source":["train_ds = dataset_google['train']\n","val_ds = dataset_google['val']\n","test_ds = dataset_google['test']\n","\n","train_ds.set_format('torch', columns=['ids', 'label', 'text_lenght'], device='cuda')\n","val_ds.set_format('torch', columns=['ids', 'label', 'text_lenght'], device='cuda')\n","test_ds.set_format('torch', columns=['ids', 'label','text_lenght'], device='cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sW1p7zmN1HPy"},"source":["# Dataloader"]},{"cell_type":"code","metadata":{"id":"DwTGehqpCujj"},"source":["# Definimos el batch size\n","bs = 64\n","# Definimos los dataloaders\n","train_dl = DataLoader(train_ds, batch_size = bs, shuffle = True)\n","val_dl = DataLoader(val_ds, batch_size = bs*2, shuffle = True)\n","test_dl = DataLoader(test_ds, batch_size = bs*2, shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NTQHdG8eo5R"},"source":["# Los dataloaderes pueden ser iterados, asi que le pedimos un elemento con 'next'\n","# simplemente para ver un ejemplo.\n","next(iter(test_dl))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0IuQJzQV1rDs"},"source":["# 3) Modelo"]},{"cell_type":"markdown","metadata":{"id":"6gR5v6KubPjf"},"source":["![](https://guide.allennlp.org/part1/your-first-model/designing-a-model-4.svg)"]},{"cell_type":"markdown","source":["------\n","\n","Sobre las capas **Embedding**:\n","\n","Buscar en un índice no es una operación que nuestros modelos redes neuronales sepan hacer. Ellas saben hacer productos de matrices y funciones de activación. Afortunadamente, podemos representar la búsqueda en un índice como un producto matricial! \n","\n","El truco es reemplazar nuestros índices de palabras por one-hot encoding! De esta manera, la mutiplicacion del one-hot vector por una matriz, nos dara la posicion de dicha matriz!\n","\n","Esta sería una forma perfectamente aceptable de construir modelos utilizando este tipo de arquitectura, excepto que utilizaría mucha más memoria y tiempo del necesario (pensar que sucederia cuando con la dimension de nuestro diccionario cuando manejamos muchos lenguajes!) \n","\n","Afortunadamente, la mayoria de las librerias de redes neuronales incluyen una capa que hace precisamente esto: indexa en un vector utilizando un entero, pero tiene su derivada calculada de tal manera que es idéntica a lo que habría sido si hubiera hecho una multiplicación de la matriz con one-hot vector. A esto esto es lo que llamamos una capa de **embedding**.\n","\n","------\n","\n"],"metadata":{"id":"_12Tym85Izde"}},{"cell_type":"markdown","source":["![](https://www.researchgate.net/profile/Manuel-Lopez-Martin/publication/349630764/figure/fig3/AS:999014610788354@1615195052671/Detail-of-the-embedding-layer-of-the-NN-implementing-the-Encoding-model-used-for-the.png)"],"metadata":{"id":"xmph9kJvJZA-"}},{"cell_type":"markdown","metadata":{"id":"QFuIqAv71KTY"},"source":["## Arquitectura"]},{"cell_type":"markdown","metadata":{"id":"z9SHZJQO1mhz"},"source":["### Configuracion\n","\n","Vamos a definir un conjunto de variables auxiliares que, como veremos en las siguientes celdas, vamos a necesitar para definir la arquitectura de nuestro modelo.\n","\n","Lo que buscamos hacer es tener un codigo donde:\n","\n","```python\n","modelo = Arquitectura (variables_de_configuracion)\n","```"]},{"cell_type":"code","metadata":{"id":"GjFrup7JR2rd"},"source":["# Vocab Size ---> Para el Embedding\n","vocab_size = tokenizer_custom.get_vocab_size()\n","# El indice del embedding que corresponde al padding\n","pad_id = tokenizer_custom.token_to_id(\"[PAD]\")\n","# Diumensiones del embedding\n","emb_dim = 150\n","# Si queremos que nuestra capa RNN/GRU sea bidireccional\n","bidirectional = True\n","# Dimensiones de Entrada y Saluda de nuestra capa recurrente\n","rnn_dim = [emb_dim, 256]\n","# Numero de capas RNN\n","num_layers = 2\n","# Numero de clases --> Para la capa de salida\n","n_classes = len(name_classes)\n","# % de Droput para regularizar\n","dropout = 0.25\n","\n","# Por comodidad le vamos a pasar todos estos\n","# parametros de config en un diccionario\n","args = {\n","    'vocab_size' : vocab_size,\n","    'pad_id' : pad_id,\n","    'embedding_dim' : emb_dim,\n","    'n_classes' : n_classes,\n","    'rnn1_dim' : rnn_dim,\n","    'bidirectional' : bidirectional,\n","    'num_layers': num_layers,\n","    'dropout': dropout\n","        }\n","args"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8Is0u8WO67x"},"source":["### Definicion"]},{"cell_type":"code","metadata":{"id":"7lj3Ytx1Cug9"},"source":["class SentimentAnalisys(nn.Module):\n","  def __init__(self,args):\n","    super(SentimentAnalisys, self).__init__()\n","    \"\"\"\n","     Se llama la funcion __init__ al momento que creemos el objeto y respondemos\n","     a la pregunta:\n","\n","     'Que partes componen al modelo?'.\n","     Es decir, inicializamos todos sus parametros a aprender.\n","     Tambien podemos definir cualquier otro tipo de variable auxiliar dentro del modelo.\n","\n","\n","     Por otro lado, en la funcion 'forward' vamos a responder a la pregunta:\n","\n","     'Como es el forward pass?. \n","     Es decir, indicamos que camino va a recorrer la informacion por las\n","     partes del modelo que previamente inicializamos.\n","\n","     #Datazo:\n","     Estrictamente hablando, tanto las partes del modelo de NN como las \n","     variables auxiliares se llaman \"atributos\", y las definimos usando el \n","     prefijo \"self.nombre_de_variable\", mientras que las funciones que definimos\n","     llaman \"metodos\".  \n","    \"\"\"\n","\n","    ###########################\n","    # Var. Auxiliares: Inicio\n","    ###########################\n","    # Guardamos variables propias del modelo para usar luego.\n","    self.vocab_size = args['vocab_size']\n","    # el token ID del padding\n","    self.pad_id = args['pad_id']\n","    # El tamaño del embedding\n","    self.embedding_dim = args['embedding_dim']\n","    # La cantidad de labels\n","    self.n_classes = args['n_classes']\n","    # la cantidad de hidden dimension de la RNN\n","    self.rnn1_dim = args['rnn1_dim']\n","    # Si vamos a usar bidericcionalidad\n","    self.bidirectional = args['bidirectional']\n","    # Cantidad de capas RNN\n","    self.num_layers = args['num_layers']\n","    # Si es bidirectional, entonces entrada a la capa fully conected (nn.Linear)\n","    # es el doble de grande\n","    self.num_directions = 2 if self.bidirectional  == True else 1\n","    ###########################\n","    # Variables Auxiliares: Fin\n","    ###########################    \n","\n","    ###########################\n","    # Partes del modelo: Inicio\n","    ###########################\n","    # 1) Definamos el embedding\n","    self.embedding = nn.Embedding(num_embeddings = self.vocab_size,\n","                                  embedding_dim = self.embedding_dim ,\n","                                  padding_idx = self.pad_id )\n","    # 2) Definimos una RNN (GRU)\n","    self.rnn1 = nn.GRU(input_size = self.rnn1_dim[0],\n","                       hidden_size = self.rnn1_dim[1],\n","                       num_layers = self.num_layers,\n","                       bidirectional = self.bidirectional,\n","                       batch_first = True )\n","    # 3)\n","    # Si el modelo tiene dos direcciones, el temaño de la ultima layer se duplica!\n","    self.linear_dim = [self.rnn1_dim[1]*self.num_directions,self.n_classes]\n","    # Definimos una capa fully-conected de salida    \n","    self.linear = nn.Linear(in_features = self.linear_dim[0],\n","                            out_features = self.linear_dim[1])\n","    # Definimos una capa de droput para regularizar\n","    self.do = nn.Dropout(args['dropout'])\n","    ###########################\n","    # Partes del modelo: Fin\n","    ###########################    \n","\n","\n","  def forward(self,x,text_lengths):\n","\n","    # 1) Capa Embedding\n","    output = self.embedding(x)\n","    \n","    # Pack_Packed:\n","    # Esta capa la hacemos simplemente para cortar el forward/backward pass\n","    # en los indices cuyo id == 'PAD'.\n","    # Doc:\n","    # https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n","    # MWE: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n","    packed_embedded = nn.utils.rnn.pack_padded_sequence(\n","        output, text_lengths.to('cpu'),\n","        batch_first=True, enforce_sorted=False)\n","\n","    # 2) Capa Recurrente\n","    packed_output, h_n = self.rnn1(packed_embedded) \n","    # Doc:\n","    # https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\n","    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n","    \n","    #################\n","    ## Bidirectional: \n","    #################\n","    # Doc: https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n","    # h_n = [num layers * num directions, batch size, hid dim]\n","    # concatenamos el hidden_state final:\n","    # * forward RNN: h_n[-2,:,:] \n","    # * backward RNN: h_n[-1,:,:]\n","    if self.bidirectional:\n","      h_n = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1)    \n","    \n","    # 3) Aplicamos dropout y la capa fully-conected.\n","    output = self.linear(self.do(h_n))\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gs1EiBw3OxB5"},"source":["### Creacion"]},{"cell_type":"code","metadata":{"id":"MSiRWxDnCued"},"source":["# Definamos el modelo, y pasemoslo a la GPU ('cuda')\n","modelo = SentimentAnalisys(args).to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_LSdPXY1wfL"},"source":["## Optimizador y Loss Function"]},{"cell_type":"code","metadata":{"id":"zo3B1slcdVQP"},"source":["#################\n","# Optimizador\n","#################\n","# Learning rate\n","learning_rate = 0.001\n","optimizer = Adam(modelo.parameters(),\n","                 lr = learning_rate,\n","                 weight_decay = 0.005) # Regularizacion\n","\n","\n","#################\n","# Loss Function\n","#################\n","# Debido a que nuestro dataset se encuentra desbalanceado,\n","# vamos a pasar un tensor indicadole como ponderaremos la loss function.\n","# 0.9: Negative , 1: Neutral, 0.3: Positive\n","class_weights = torch.FloatTensor([0.9,1,0.3]).to(device)\n","# Definimos la loss function y le pasamos los pesos!\n","loss_function = nn.CrossEntropyLoss(weight=class_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZPjCW2b3jdPK"},"source":["# Vamos a generarnos un diccionario donde iremos guardando los resultados\n","# del entrenamiento\n","history = {'train':{'loss':[],'acc': []},\n","           'val':{'loss': [],'acc': []}}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0QGO5FN10xi"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"fgOtDvORbk7v"},"source":["![](https://drive.google.com/uc?id=1d7Rq5FsmQNWcP8T1KGi_TodL49jCEKnY)\n","\n","![](https://guide.allennlp.org/part1/your-first-model/designing-a-model-5.svg)"]},{"cell_type":"code","metadata":{"id":"DohIzJVV0WZ6"},"source":["# Definimos la cantidad de epochs a entrenar\n","epochs = 25\n","# Cantidad de muestras que hay en cada dataset,\n","# Esto lo necesitamos para que la comparacion de la loss de entrenamiento y validacion\n","# sea valida\n","normalizer_train = len(train_dl.dataset)\n","normalizer_val = len(val_dl.dataset)\n","normalizer_test = len(test_dl.dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNXL1CdXdVKQ"},"source":["# Vamos a crear 4 variables para ir haciendo seguimiento de la loss\n","# y del accuracy duante cada epoc\n","epoch_loss_train = 0\n","epoch_loss_val = 0\n","accuracy_train = 0 \n","accuracy_val = 0\n","\n","pbar1 = tqdm(range(epochs),unit=\" epoch\")\n","for epoch in pbar1:\n","  # Aqui creamos la barra y configuramos que mensajes queremos que aparezcan\n","  # durante el entrenamiento\n","  pbar1.set_postfix({\n","      'Loss_t': epoch_loss_train,\n","      'Acc_t': accuracy_train,\n","      'Loss_v':epoch_loss_val,\n","      'Acc_v': accuracy_val})\n","  \n","  # Reseteamos las variables debido al for loop\n","  epoch_loss_train , epoch_loss_val = 0, 0\n","  accuracy_train , accuracy_val = 0, 0 \n","  ###############################  \n","  # TRAINING   \n","  ###############################\n","  with tqdm(total=len(train_dl)+len(val_dl), unit=\" batch\",leave=False) as pbar2:\n","    # Cantidad de labels acertados  \n","    correct = 0\n","    # Pasamos el modelo a modo 'train' (activa los dropout/batchnorm)\n","    modelo.train()\n","    # Para cada batch del dataloader\n","    for batch in train_dl:\n","      # Actualizamos el contador de batches de la barra de progreso\n","      pbar2.update(1)\n","      # x: nuestros ids\n","      x = batch['ids']\n","      x_l = batch['text_lenght']\n","      # y_true: el label correcto\n","      y_true = batch['label']\n","      # Limpiamos los gradientes que observa el optimizador\n","      # si no lo hacemos, se van a ir acumulando y es incorrecto\n","      optimizer.zero_grad()\n","      # Generamos una prediccion\n","      y_hat = modelo(x,x_l)\n","      # Calculamos la loss w.r.t el true value\n","      loss = loss_function(y_hat,y_true)\n","      # Calculamos los gradientes del modelo w.r.t la loss\n","      loss.backward()\n","      # Aplicamos los gradientes a los parametros (damos un step)\n","      optimizer.step()\n","      # Obtenemos el label predicho\n","      _, predicted = torch.max(y_hat.data, 1)\n","      # Acumulamos la loss\n","      epoch_loss_train += loss.item()\n","      # Cantidad de labels correctamente predichos\n","      correct_i = (predicted == y_true).sum().item() \n","      correct += correct_i\n","      pbar2.set_postfix({\n","              'Loss_t:': loss.item() / x.size()[0],\n","              'Acc_t': correct_i / (x.size()[0])})      \n","\n","    # una vez que termina una epoch de training, normalizamos loss y acc\n","    # por la cantidad de samples que habia en el set de entrenamiento\n","    epoch_loss_train = epoch_loss_train / normalizer_train\n","    accuracy_train = 100 * correct / normalizer_train\n","\n","    ###############################  \n","    # Validation   \n","    ###############################\n","    # Desactivamos los dropout/batch_norm\n","    modelo.eval()\n","    # volvemos a inicializar el contador para el accuracy\n","    correct = 0\n","    # Como ahora estamos testeando, no necesitamos computar gradiente\n","    # (desactiva funciones y operaciones sobre el grafo computacional)\n","    with torch.inference_mode():\n","      for batch in val_dl:\n","        pbar2.update(1)\n","        x = batch['ids']\n","        x_l = batch['text_lenght']\n","        y_true = batch['label']\n","        y_hat = modelo(x,x_l)\n","        val_loss = loss_function(y_hat,y_true)\n","        _, predicted = torch.max(y_hat.data, 1)\n","        correct += (predicted == y_true).sum().item()\n","        epoch_loss_val += val_loss.item()\n","\n","    #Estimamos la loss y el acc w.r.t el set de testeo\n","    epoch_loss_val = epoch_loss_val / normalizer_val\n","    accuracy_val = 100 * correct / normalizer_val\n","\n","    # Actualizamos nuestro historial\n","    history['train']['loss'].append(epoch_loss_train)\n","    history['val']['loss'].append( epoch_loss_val)\n","    history['train']['acc'].append(accuracy_train)\n","    history['val']['acc'].append(accuracy_val)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhUWlm4zAMJr"},"source":["#Loss\n","train_loss_h = history['train']['loss']\n","val_loss_h = history['val']['loss']\n","# Acc\n","train_acc_h = history['train']['acc']\n","val_acc_h = history['val']['acc']\n","lepochs = range(1, len(train_loss_h) + 1)\n","# Plot\n","fig, axs = plt.subplots(1,2,figsize=(16,6))\n","axs[0].plot(lepochs, train_loss_h, 'b', label='Train loss')\n","axs[0].plot(lepochs, val_loss_h, 'r', label='Val loss')\n","axs[0].set_title('Training and validation Loss',fontsize=20)\n","axs[0].legend(fontsize=16)\n","axs[1].plot(lepochs, train_acc_h, 'b', label='Train Accuracy')\n","axs[1].plot(lepochs, val_acc_h, 'r', label='Validation Accuracy')\n","axs[1].set_title('Training and validation Accuracy',fontsize=20)\n","axs[1].legend(fontsize=16)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V302Agc-NGOy"},"source":["## Save and load"]},{"cell_type":"code","metadata":{"id":"aq8MqXnuK9ge"},"source":["#torch.save(modelo.state_dict(), root_path+'NLP.pt')\n","modelo = SentimentAnalisys(args).to('cuda')\n","modelo.load_state_dict(torch.load(root_path+'NLP.pt'))\n","modelo.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DptvXYoKT7yF"},"source":["## Ejemplos"]},{"cell_type":"code","metadata":{"id":"grAOHoMJ3X-a"},"source":["text_proposed = \"this is the worst app I have ever downloaded in my life \"\n","input_test = tokenizer_custom.encode(text_proposed)\n","input_lenght = [np.sum(np.array(input_test.special_tokens_mask)==0)+2]\n","input_lenght = torch.LongTensor(input_lenght)\n","input_test = torch.LongTensor(input_test.ids).to(device).unsqueeze(0)\n","prediction = modelo(input_test,input_lenght).to('cpu')\n","prediction_prob = softmax(prediction.detach().numpy())[0]\n","prediction_id = np.flip(np.argsort(prediction_prob))\n","pd.DataFrame({'Class': np.array(name_classes)[prediction_id],\n","              'Probability' : prediction_prob[prediction_id]*100})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldOVyPCwU6Zl"},"source":["**Veamos ejemplos aleatorios del dataset de testeo**"]},{"cell_type":"code","metadata":{"id":"fb6nrS1hRkH-"},"source":["rnd_idx = np.random.randint(len(test_ds))\n","print(tokenizer_custom.decode(test_ds[rnd_idx]['ids'].to('cpu').numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5uAmzwNbk5F"},"source":["print(name_classes[test_ds[rnd_idx]['label']])\n","input_test = test_ds[rnd_idx]['ids'].unsqueeze(0)\n","input_lenght = test_ds[rnd_idx]['text_lenght'].unsqueeze(0)\n","prediction = modelo(input_test,input_lenght).to('cpu')\n","prediction_prob = softmax(prediction.detach().numpy())[0]\n","prediction_id = np.flip(np.argsort(prediction_prob))\n","pd.DataFrame({'Class': np.array(name_classes)[prediction_id],\n","              'Probability' : prediction_prob[prediction_id]*100})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y9ybuaXeH3ES"},"source":["# Evaluation\n","\n","Creamos una funcion para que evalua sobbre el test set."]},{"cell_type":"code","metadata":{"id":"YhJfuOLoVu54"},"source":["def evaluate(modelo,dataloader):\n","    # Modelo en modo evaluation\n","    # Desactiva el dropout\n","    modelo.eval()\n","    # Variables donde guardar resultados\n","    test_loss_total = 0\n","    y_pred, y_true = [], []\n","    correct = 0\n","    with torch.inference_mode():        \n","      for batch in tqdm(dataloader):\n","        x = batch['ids']\n","        x_l = batch['text_lenght']\n","        y = batch['label']\n","        y_hat = modelo(x,x_l)\n","        loss = loss_function(y_hat,y)\n","        test_loss_total += loss.item()\n","        _, predicted = torch.max(y_hat.data, 1)\n","        predicted = predicted.detach().cpu().numpy()\n","        label_ids = y.cpu().numpy()\n","        y_pred.append(predicted)\n","        y_true.append(label_ids)\n","    \n","    test_loss = test_loss_total/len(dataloader) \n","    y_pred = np.concatenate(y_pred, axis=0)\n","    y_true = np.concatenate(y_true, axis=0)\n","            \n","    return test_loss, y_pred, y_true"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xvu_qOSHzhe"},"source":["test_loss, y_pred, y_true = evaluate(modelo,test_dl)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pOGS24tVZrdS"},"source":["## Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"pJbAlA8qH0Fw"},"source":["con_mat = confusion_matrix(y_true,y_pred)\n","acc_test = accuracy_score(y_true,y_pred)\n","print(f\"Test set Accuracy: {acc_test} %\")\n","\n","df_cm = pd.DataFrame(con_mat, index = [i for i in name_classes],\n","                  columns = [i for i in name_classes])\n","\n","plt.figure(figsize = (10,7))\n","sns.heatmap(df_cm, annot=True,fmt='g')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mHcIQeCfuJyr"},"source":["# Preguntas?"]},{"cell_type":"markdown","metadata":{"id":"6fqNNeMMuDeN"},"source":["![](https://preview.redd.it/t87gswsbmnq41.jpg?width=640&crop=smart&auto=webp&s=b2777564c452e6de90edd14ef57b8740632da573)"]}]}